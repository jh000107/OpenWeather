{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "junhuichosynapse"
		},
		"junhuichosynapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'junhuichosynapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:junhuichosynapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"junhuichosynapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://junhuisa.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/ds598sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/junhuichosynapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('junhuichosynapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/junhuichosynapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('junhuichosynapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Drop Script')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Drop external tables\nDROP EXTERNAL TABLE ExternalAggAQI;\nDROP EXTERNAL TABLE ExternalAggPollutants;\nDROP EXTERNAL TABLE ExternalAggTempExtremes;\nDROP EXTERNAL TABLE ExternalAggWeather;\nDROP EXTERNAL TABLE ExternalAggWeatherConditions;\nDROP EXTERNAL TABLE ExternalAirPollution;\nDROP EXTERNAL TABLE ExternalHighPollutionEvents;\nDROP EXTERNAL TABLE ExternalWeather;\nGO\n\n-- Drop regular tables\nDROP TABLE AggAQI;\nDROP TABLE AggPollutants;\nDROP TABLE AggTempExtremes;\nDROP TABLE AggWeather;\nDROP TABLE AggWeatherConditions;\nDROP TABLE DimDateTime;\nDROP TABLE DimLocation;\nDROP TABLE DimAirPollution;\nDROP TABLE DimWeatherCondition;\nDROP TABLE FactWeather;\nDROP TABLE HighPollutionEvents;\nDROP TABLE DimDate;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ds598sqlpool",
						"poolName": "ds598sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ExternalTables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE EXTERNAL TABLE ExternalWeather ( --positionality matters for dedicated tables! Ensure its the correct order of datatypes and correct number\n    clouds_all SMALLINT,\n    feels_like_K FLOAT,\n    humidity SMALLINT,\n    pressure SMALLINT,\n    temp_K FLOAT,\n    temp_max_K FLOAT,\n    temp_min_K FLOAT,\n    rain_1h REAL,\n    snow_1h REAL,\n    wind_deg SMALLINT,\n    wind_gust FLOAT,\n    wind_speed FLOAT,\n    corrected_timestamp INT,\n    location NVARCHAR(100),\n    date_time DATETIME,\n    id NVARCHAR(100),\n    temp_C FLOAT,\n    temp_min_C FLOAT,\n    temp_max_C FLOAT,\n    feels_like_C FLOAT,\n    temp_min_F FLOAT,\n    temp_max_F FLOAT,\n    feels_like_F FLOAT,\n    lon FLOAT,\n    lat FLOAT,\n    weather_id_value INT,\n    weather_main_value NVARCHAR(100),\n    weather_description_value NVARCHAR(100),\n    weather_icon_value NVARCHAR(100),\n    temp_F FLOAT\n)\nWITH (\n    LOCATION = 'gold/processed_weather/processed_weather.parquet', --may be different based on your storage location\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAirPollution (\n    lon FLOAT,                          -- Longitude\n    lat FLOAT,                          -- Latitude\n    aqi INT,                            -- Air Quality Index (integer)\n    co FLOAT,                           -- Carbon monoxide level\n    no FLOAT,                           -- Nitric oxide level\n    no2 FLOAT,                          -- Nitrogen dioxide level\n    o3 FLOAT,                           -- Ozone level\n    so2 FLOAT,                          -- Sulfur dioxide level\n    pm2_5 FLOAT,                        -- Particulate matter 2.5 level\n    pm10 FLOAT,                         -- Particulate matter 10 level\n    nh3 FLOAT,                          -- Ammonia level\n    corrected_timestamp INT,            -- Corrected timestamp (integer)\n    location NVARCHAR(100),             -- Location as a string\n    id NVARCHAR(100),                   -- String identifier\n    date_time DATETIME2,                -- Date and time with timestamp precision\n    o3_8hr FLOAT,                       -- Ozone level (8-hour average)\n    o3_1hr FLOAT,                       -- Ozone level (1-hour average)\n    pm2_5_24hr FLOAT,                   -- PM2.5 level (24-hour average)\n    pm10_24hr FLOAT,                    -- PM10 level (24-hour average)\n    co_8hr FLOAT,                       -- CO level (8-hour average)\n    so2_1hr FLOAT,                      -- SO2 level (1-hour average)\n    so2_24hr FLOAT,                     -- SO2 level (24-hour average)\n    no2_1hr FLOAT,                      -- NO2 level (1-hour average)\n    us_aqi INT                          -- U.S. Air Quality Index (integer)\n)\nWITH (\n    LOCATION = 'gold/processed_air_pollution/processed_air_pollution.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAggWeatherConditions (\n\tdate DATE,\n    weather_main_value VARCHAR(100),\n    count INT\n)\nWITH (\n    LOCATION = 'gold/agg_weather_conditions/agg_weather_conditions.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAggTempExtremes (\n    date Date,\n    max_temp_F FLOAT,\n    min_temp_F FLOAT\n)\nWITH (\n    LOCATION = 'gold/agg_temp_extremes/agg_temp_extremes.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAggAQI (\n\tdate Date,\n    avg_us_aqi FLOAT\n)\nWITH (\n    LOCATION = 'gold/agg_aqi/agg_aqi.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalHighPollutionEvents (\n    date Date,\n    high_pollutant_events INT\n)\nWITH (\n    LOCATION = 'gold/agg_high_pollution_events/agg_high_pollution_events.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAggWeather (\n    date Date,\n    avg_temp_F FLOAT,\n    avg_humidity FLOAT,\n    max_temp_F FLOAT,\n    min_temp_F FLOAT,\n    weather_records INT\n)\nWITH (\n    LOCATION = 'gold/agg_weather/agg_weather.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\nCREATE EXTERNAL TABLE ExternalAggPollutants (\n    date Date,\n    avg_co FLOAT,\n    avg_no2 FLOAT,\n    avg_o3 FLOAT,\n    avg_so2 FLOAT,\n    avg_pm2_5 FLOAT,\n    avg_pm10 FLOAT\n)\nWITH (\n    LOCATION = 'gold/agg_pollutants/agg_pollutants.parquet',\n    DATA_SOURCE = MyDataSource,\n    FILE_FORMAT = ParquetFileFormat\n);\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ds598sqlpool",
						"poolName": "ds598sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InternalTabels')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Create FactWeather table with matching data types (internal sql table)\nCREATE TABLE FactWeather (\n    id NVARCHAR(100),                             -- String identifier to match ExternalWeather's NVARCHAR(100)\n    date_time DATETIME2,                          -- Higher precision datetime to match ExternalWeather's DATETIME2 usage\n    date DATE,        \n    location NVARCHAR(100),                       -- Shorter NVARCHAR(100) to match ExternalWeather's location column\n    humidity SMALLINT,                            -- Small integer to match ExternalWeather's SMALLINT for humidity\n    pressure SMALLINT,                            -- Small integer to match ExternalWeather's SMALLINT for pressure\n    clouds_all SMALLINT,                          -- Small integer to match ExternalWeather's SMALLINT for clouds_all\n    temp_K FLOAT,                                 -- Float to match ExternalWeather's temp_K\n    feels_like_K FLOAT,                           -- Float to match ExternalWeather's feels_like_K\n    temp_max_K FLOAT,                             -- Float to match ExternalWeather's temp_max_K\n    temp_min_K FLOAT,                             -- Float to match ExternalWeather's temp_min_K\n    temp_C FLOAT,                                 -- Float to match ExternalWeather's temp_C\n    feels_like_C FLOAT,                           -- Float to match ExternalWeather's feels_like_C\n    temp_max_C FLOAT,                             -- Float to match ExternalWeather's temp_max_C\n    temp_min_C FLOAT,                             -- Float to match ExternalWeather's temp_min_C\n    temp_F FLOAT,                                 -- Float to match ExternalWeather's temp_F\n    feels_like_F FLOAT,                           -- Float to match ExternalWeather's feels_like_F\n    temp_max_F FLOAT,                             -- Float to match ExternalWeather's temp_max_F\n    temp_min_F FLOAT,                             -- Float to match ExternalWeather's temp_min_F\n    weather_combined_value NVARCHAR(200)          -- NVARCHAR to match ExternalWeather, increased length for combined data\n);\nGO\n\n-- Insert data into FactWeather\n-- wind columns might be null if you didnt ingest them, so remove them from the insert\nINSERT INTO FactWeather (id, date_time, date, location, humidity, pressure, clouds_all, temp_K, feels_like_K, temp_max_K, temp_min_K, temp_C, feels_like_C, temp_max_C, temp_min_C, temp_F, feels_like_F, temp_max_F, temp_min_F, weather_combined_value)\nSELECT \n    id,\n    date_time, \n    CAST(date_time AS DATE) AS date,\n    location,\n    humidity, \n    pressure, \n    clouds_all, \n    temp_K, \n    feels_like_K, \n    temp_max_K, \n    temp_min_K,\n    temp_C,\n    feels_like_C,\n    temp_max_C,\n    temp_min_C,\n    temp_F,\n    feels_like_F,\n    temp_max_F,\n    temp_min_F,\n    CONCAT(weather_id_value, '_', weather_icon_value) AS weather_combined_value\nFROM \n    ExternalWeather;\nGO\n\n-- Create and load DimAirPollution table\nCREATE TABLE DimAirPollution (\n    id NVARCHAR(100),\n    aqi NVARCHAR(100),\n    co FLOAT,\n    no FLOAT,\n    no2 FLOAT,\n    o3 FLOAT,\n    so2 FLOAT,\n    pm2_5 FLOAT,\n    pm10 FLOAT,\n    nh3 FLOAT\n);\nGO\n\nINSERT INTO DimAirPollution (id, aqi, co, no, no2, o3, so2, pm2_5, pm10, nh3)\nSELECT DISTINCT \n    id, \n    aqi, \n    co, \n    no, \n    no2, \n    o3, \n    so2, \n    pm2_5, \n    pm10, \n    nh3\nFROM \n    ExternalAirPollution;\n\n-- Create and load DimDateTime table\nCREATE TABLE DimDateTime (\n-- A dimension table that breaks down date and time components.\n-- Contains detailed temporal information such as year, month, day, hour, minute, and whether the date is a weekend.\n    date_time DATETIME,\n    date DATE,\n    year INT,\n    month INT,\n    day INT,\n    hour INT,\n    minute INT,\n    second INT,\n    quarter INT,\n    week INT,\n    day_of_week INT,\n    day_name VARCHAR(10),\n    month_name VARCHAR(10),\n    is_weekend BIT\n);\nGO\n\nINSERT INTO DimDateTime (date_time, date, year, month, day, hour, minute, second, quarter, week, day_of_week, day_name, month_name, is_weekend)\nSELECT DISTINCT \n    date_time,\n    CAST(date_time AS DATE) AS date,\n    DATEPART(YEAR, date_time) AS year,\n    DATEPART(MONTH, date_time) AS month,\n    DATEPART(DAY, date_time) AS day,\n    DATEPART(HOUR, date_time) AS hour,\n    DATEPART(MINUTE, date_time) AS minute,\n    DATEPART(SECOND, date_time) AS second,\n    DATEPART(QUARTER, date_time) AS quarter,\n    DATEPART(WEEK, date_time) AS week,\n    DATEPART(WEEKDAY, date_time) AS day_of_week,\n    DATENAME(WEEKDAY, date_time) AS day_name,\n    DATENAME(MONTH, date_time) AS month_name,\n    CASE WHEN DATEPART(WEEKDAY, date_time) IN (1, 7) THEN 1 ELSE 0 END AS is_weekend\nFROM \n    ExternalWeather;\nGO\n\n-- Create and load DimWeatherCondition table\nCREATE TABLE DimWeatherCondition (\n-- A dimension table for weather condition details.\n-- Stores information about different weather conditions, including main weather categories and descriptions, along with a combined weather condition value.\n    weather_id_value VARCHAR(100),\n    weather_icon_value VARCHAR(10),\n    weather_main_value VARCHAR(100),\n    weather_description_value VARCHAR(100),\n    weather_combined_value VARCHAR(110)\n);\nGO\n\nINSERT INTO DimWeatherCondition\nSELECT DISTINCT \n    weather_id_value, \n    weather_icon_value, \n    weather_main_value, \n    weather_description_value,\n    CONCAT(weather_id_value, '_', weather_icon_value) AS weather_combined_value\nFROM \n    ExternalWeathr;\nGO\n\n-- Create and load DimDate table\nCREATE TABLE DimDate (\n-- A dimension table focused on date attributes.\n-- Provides date details like year, month, day, quarter, and week, along with names for days and months, and a weekend indicator.\n    date DATE,\n    year INT,\n    month INT,\n    day INT,\n    quarter INT,\n    week INT,\n    day_of_week INT,\n    day_name VARCHAR(10),\n    month_name VARCHAR(10),\n    is_weekend BIT\n);\nGO\n\n-- Populate DimDate Table from ExternalWeather\nINSERT INTO DimDate (date_time, year, month, day, quarter, week, day_of_week, day_name, month_name, is_weekend)\nSELECT DISTINCT\n    CAST(date_time AS DATE) AS date,\n    DATEPART(YEAR, date_time) AS year,\n    DATEPART(MONTH, date_time) AS month,\n    DATEPART(DAY, date_time) AS day,\n    DATEPART(QUARTER, date_time) AS quarter,\n    DATEPART(WEEK, date_time) AS week,\n    DATEPART(WEEKDAY, date_time) AS day_of_week,\n    DATENAME(WEEKDAY, date_time) AS day_name,\n    DATENAME(MONTH, date_time) AS month_name,\n    CASE WHEN DATEPART(WEEKDAY, date_time) IN (1, 7) THEN 1 ELSE 0 END AS is_weekend\nFROM\n    ExternalWeather;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ds598sqlpool",
						"poolName": "ds598sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Resource Creation')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Create external data source to connect to your container\nCREATE EXTERNAL DATA SOURCE MyDataSource \nWITH (\n    LOCATION = 'https://junhuisa.dfs.core.windows.net/blob1'\n);\nGO\n\n-- Create external file format\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat\nWITH (\n    FORMAT_TYPE = PARQUET\n);\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Validate')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- SELECT TOP 10 * FROM FactWeather;\n-- SELECT TOP 10 * FROM DimLocation;\n-- SELECT TOP 10 * FROM DimAirPollution;\n-- SELECT TOP 10 * FROM DimWeatherCondition;\nSELECT TOP 10 * FROM DimDateTime;\n-- SELECT TOP 10 * FROM DimDate;\n-- SELECT TOP 10 * FROM AggWeather;\n-- SELECT TOP 10 * FROM AggWeatherConditions;\n-- SELECT TOP 10 * FROM AggTempExtremes;\n-- SELECT TOP 10 * FROM AggAQI;\n-- SELECT TOP 10 * FROM AggPollutants;\n-- SELECT TOP 10 * FROM HighPollutionEvents;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ds598sqlpool",
						"poolName": "ds598sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpenWeather EDA')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "openweather",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "30db9b0f-8668-4d21-82ab-eea2caca13d7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/34f89b90-47ae-412e-8758-934b3c3ef8f7/resourceGroups/junhuicho-DS598/providers/Microsoft.Synapse/workspaces/junhuichosynapse/bigDataPools/openweather",
						"name": "openweather",
						"type": "Spark",
						"endpoint": "https://junhuichosynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/openweather",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OpenWeather Data EDA"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Configuration\n",
							"\n",
							"- Synapse is connected to my Github, so it is necessary to secure my API key. Key Vault access is only given to my personal account.\n",
							"    - It is smart enough that it prevents you from connecting Synapse to Github if there is a key exposed in the Synapse environment.\n",
							"    - Secured Key: Storage Account Key\n",
							"- Connect to my storage account where the OpenWeather data is stored."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# Define Key Vault Name\n",
							"key_vault_name = \"openweather-junhui\"\n",
							"secret_name = \"storage-key\"  \n",
							"\n",
							"storage_account_name = \"junhuisa\"\n",
							"storage_account_key = mssparkutils.credentials.getSecret(key_vault_name, secret_name)\n",
							"\n",
							"spark.conf.set(\n",
							"    \"fs.azure.account.key.\" + storage_account_name + \".dfs.core.windows.net\",\n",
							"    storage_account_key\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read data directly from ADLS Gen2\n",
							"container_name = \"blob1\"\n",
							"air_pollution_folder_path = \"bronze/historical_airpollution\"  # Folder, not a single file\n",
							"historical_weather_folder_path = \"bronze/historical_weather\"\n",
							"\n",
							"# Read all JSON files in the folder\n",
							"air_pollution_df = spark.read.json(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{air_pollution_folder_path}\")\n",
							"historical_weather_df = spark.read.json(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{historical_weather_folder_path}\")"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Observation\n",
							"\n",
							"Taking a look at the raw data."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Historical Weather Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Show the DataFrame\n",
							"\n",
							"historical_weather_df.head()"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Print schema\n",
							"\n",
							"historical_weather_df.printSchema()"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Count the number of rows to see if the data is loaded correctly\n",
							"# Ideally, there should be one row for each timestamp\n",
							"\n",
							"historical_weather_df.count()"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Air Pollution Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"air_pollution_df.head()"
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"air_pollution_df.printSchema()"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"air_pollution_df.count()"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Restructuring DataFrames"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Historical Weather Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import explode\n",
							"\n",
							"# Flatten the 'list' column\n",
							"historical_weather_df = historical_weather_df.withColumn(\"list\", explode(historical_weather_df[\"list\"]))\n",
							"print(\"Number of rows after flattening the 'list':\", historical_weather_df.count())\n",
							"historical_weather_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Instead of having a merged column of all fields,\n",
							"# extract each field out to have a its own column\n",
							"\n",
							"historical_weather_df = historical_weather_df.selectExpr(\n",
							"    \"calctime\", \n",
							"    \"city_id\", \n",
							"    \"cnt\", \n",
							"    \"cod\", \n",
							"    \"message\", \n",
							"    \"list.dt\",\n",
							"    \"list.main.temp as temp\",\n",
							"    \"list.main.feels_like as feels_like\",\n",
							"    \"list.main.humidity as humidity\",\n",
							"    \"list.main.pressure as pressure\",\n",
							"    \"list.main.temp_min as temp_min\",\n",
							"    \"list.main.temp_max as temp_max\",\n",
							"    \"list.wind.speed as wind_speed\",\n",
							"    \"list.wind.deg as wind_deg\",\n",
							"    \"list.wind.gust as wind_gust\",\n",
							"    \"list.clouds.all as clouds_all\",\n",
							"    \"list.weather.description as weather_description\",\n",
							"    \"list.weather.icon as weather_icon\",\n",
							"    \"list.weather.id as weather_id\",\n",
							"    \"list.weather.main as weather_main\",\n",
							"    \"list.rain.1h as rain_1h\",\n",
							"    \"list.snow.1h as snow_1h\"\n",
							")\n",
							"\n",
							"historical_weather_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Air Pollution Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"air_pollution_df = air_pollution_df.withColumn(\"list\", explode(air_pollution_df[\"list\"]))\n",
							"print(\"Number of rows after flattening the 'list':\", air_pollution_df.count())\n",
							"air_pollution_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"air_pollution_df = air_pollution_df.selectExpr(\n",
							"    \"coord\",\n",
							"    \"list.dt as dt\",\n",
							"    \"list.main.aqi as aqi\",\n",
							"    \"list.components.co as co\",\n",
							"    \"list.components.no as no\",\n",
							"    \"list.components.no2 as no2\",\n",
							"    \"list.components.o3 as o3\",\n",
							"    \"list.components.so2 as so2\",\n",
							"    \"list.components.pm2_5 as pm2_5\",\n",
							"    \"list.components.pm10 as pm10\",\n",
							"    \"list.components.nh3 as nh3\"\n",
							")\n",
							"\n",
							"air_pollution_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Cleaning"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Historical Weather Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Copy in order to preserve the raw data\n",
							"cleaned_weather_df = historical_weather_df.select('*')\n",
							"\n",
							"# Drop meaningless columns and order by timestamp\n",
							"columns_to_drop = ['calctime', 'city_id', 'cnt', 'cod', 'message']\n",
							"cleaned_weather_df = cleaned_weather_df.drop(*columns_to_drop).orderBy(\"dt\")\n",
							"\n",
							"cleaned_weather_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert to pandas for further analysis\n",
							"cleaned_weather_df = cleaned_weather_df.toPandas()\n",
							"\n",
							"cleaned_weather_df.head(10)"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Count the number of null values for each column\n",
							"cleaned_weather_nan_count = cleaned_weather_df.isnull().sum()\n",
							"\n",
							"print(cleaned_weather_nan_count)\n",
							"\n",
							"# According to the OpenWeather API documentation,\n",
							"# wind_gust - not described. A sudden increase in wind speed\n",
							"# rain_1h - Rain volume for the last 1 hour\n",
							"# snow_1h - Snow volume for the last 1 hour\n",
							"\n",
							"# Therefore, we can assume that having null values for these fields basically means 0\n",
							"# To confirm this, check whether there is any data with these values of 0\n",
							"print(\"Number of rows with rain_1h of value 0:\", (cleaned_weather_df['rain_1h'] == 0).sum())\n",
							"print(\"Number of rows with snow_1h of value 0:\", (cleaned_weather_df['snow_1h'] == 0).sum())\n",
							"print(\"Number of rows with wind_gust of value 0:\", (cleaned_weather_df['wind_gust'] == 0).sum())"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Fill the null values with 0\n",
							"cleaned_weather_df.fillna(0, inplace=True)\n",
							"\n",
							"print(cleaned_weather_df.isnull().sum())"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# According to OpenWeather API documentation, default temperature unit is Kelvin\n",
							"# Convert Kelvin into Fahrenheit for better insight\n",
							"\n",
							"cleaned_weather_df['temp'] = (cleaned_weather_df['temp'] - 273.15) * 9/5 + 32\n",
							"cleaned_weather_df['feels_like'] = (cleaned_weather_df['feels_like'] - 273.15) * 9/5 + 32\n",
							"cleaned_weather_df['temp_max'] = (cleaned_weather_df['temp_max'] - 273.15) * 9/5 + 32\n",
							"cleaned_weather_df['temp_min'] = (cleaned_weather_df['temp_min'] - 273.15) * 9/5 + 32\n",
							"cleaned_weather_df.head(10)"
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Air Pollution Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cleaned_air_pollution_df = air_pollution_df.select('*')\n",
							"\n",
							"# Drop meaningless columns and order by timestamp\n",
							"columns_to_drop = ['coord']\n",
							"cleaned_air_pollution_df = cleaned_air_pollution_df.drop(*columns_to_drop).orderBy(\"dt\")\n",
							"\n",
							"cleaned_air_pollution_df.show(10, truncate=False)"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cleaned_air_pollution_df = cleaned_air_pollution_df.toPandas()\n",
							"\n",
							"cleaned_air_pollution_df.head(10)"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cleaned_air_pollution_nan_count = cleaned_air_pollution_df.isnull().sum()\n",
							"\n",
							"print(cleaned_air_pollution_nan_count)"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Statistics"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Historical Weather Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Generate basic statistics\n",
							"\n",
							"cleaned_weather_df.describe()"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Air Pollution Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cleaned_air_pollution_df.describe()"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here, we can observe that there are obvious errors for 'no2' and 'pm10'. It is better not to change too much of the data, but ovbious errors like these can be replaced."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"Obvious errors in 'no2':\")\n",
							"print(cleaned_air_pollution_df[cleaned_air_pollution_df['no2'] < 0])\n",
							"\n",
							"print(\"Obvious erros in 'pm10':\")\n",
							"print(cleaned_air_pollution_df[cleaned_air_pollution_df['pm10'] < 0])"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Replace -9999s with 0\n",
							"cleaned_air_pollution_df.replace(-9999, 0, inplace=True)\n",
							"\n",
							"cleaned_air_pollution_df.describe()"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Correlation Matrix Between Two Datasets\n",
							"\n",
							"In order to draw a correlation matrix between these the two datasets, two dataframes are inner-joined with a common column 'dt'."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\n",
							"\n",
							"joined_df = pd.merge(cleaned_weather_df, cleaned_air_pollution_df, on=\"dt\", how=\"inner\")\n",
							"\n",
							"joined_df.head(10)"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Check shape to see how many rows are in common and features are correctly joined\n",
							"print(joined_df.shape)"
						],
						"outputs": [],
						"execution_count": 109
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Only select numercial features for correlation matrix\n",
							"numerical_df = joined_df.select_dtypes(include=['number'])\n",
							"\n",
							"numerical_df.head(10)"
						],
						"outputs": [],
						"execution_count": 110
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import seaborn as sns\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"corr_matrix = numerical_df.corr()\n",
							"\n",
							"plt.figure(figsize=(12, 8))\n",
							"sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Visualizations"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Time Series: Temperature, Feels Like"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import matplotlib.dates as mdates\n",
							"\n",
							"plt.figure(figsize=(12, 6))\n",
							"plt.plot(pd.to_datetime(cleaned_weather_df['dt'], unit='s'), cleaned_weather_df['temp'], label='Temperature', linestyle='-', alpha=0.7)\n",
							"plt.plot(pd.to_datetime(cleaned_weather_df['dt'], unit='s'), cleaned_weather_df['feels_like'], label='Feels Like', linestyle='-', alpha=0.2)\n",
							"\n",
							"# Format x-axis so that it shows per month interval\n",
							"plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
							"plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
							"plt.xticks(rotation=45)\n",
							"\n",
							"\n",
							"plt.xlabel('Time')\n",
							"plt.ylabel('Temperature (Fahrenheit)')\n",
							"plt.title('Temperature Over Time')\n",
							"plt.legend()"
						],
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Time Series: Difference in Actual and Apprent Temperature\n",
							"\n",
							"The difference is computed by -1 * ('temp' - 'feels_like'). The negation is applied to make the graph easier to read. For example, 10 means \"It feels like 10F more than it actually is\"."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create the plot\n",
							"plt.figure(figsize=(12, 6))\n",
							"plt.plot(pd.to_datetime(cleaned_weather_df['dt'], unit='s'), -1 * (cleaned_weather_df['temp'] - cleaned_weather_df['feels_like']), label='Temperature', linestyle='-', alpha=0.7)\n",
							"\n",
							"# Format x-axis\n",
							"plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Year-Month format\n",
							"plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Show every month\n",
							"plt.xticks(rotation=45)  # Rotate for better readability\n",
							"\n",
							"# Labels and title\n",
							"plt.xlabel('Time (Year-Month)')\n",
							"plt.ylabel('Temperature Gap')\n",
							"plt.title('Difference in Actual and Apparent (Feels Like) Over Time')\n",
							"plt.legend()"
						],
						"outputs": [],
						"execution_count": 113
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Distribution: Air Quality Index\n",
							"\n",
							"Instead of looking at the distribution of rather continuous variables like temperature, it seems more reasonable to observe the distribution of air quality index because it has only 5 classes ranging from [1,5]."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"aqi_counts = cleaned_air_pollution_df['aqi'].value_counts().sort_index()\n",
							"\n",
							"plt.figure(figsize=(8, 5))\n",
							"plt.bar(aqi_counts.index, aqi_counts.values, color='blue', alpha=0.7, edgecolor='black')\n",
							"\n",
							"plt.xlabel('AQI Category')\n",
							"plt.ylabel('Count')\n",
							"plt.title('Distribution of Air Quality Index (AQI)')\n",
							"plt.xticks([1, 2, 3, 4, 5])  # Ensure x-axis has correct AQI classes\n",
							"plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Humidity vs Temperature"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"plt.figure(figsize=(8, 5))\n",
							"plt.scatter(cleaned_weather_df['temp'], cleaned_weather_df['humidity'], alpha=0.5, color='blue')\n",
							"\n",
							"\n",
							"plt.xlabel('Temperature (Fahrenheit)')\n",
							"plt.ylabel('Humidity (%)')\n",
							"plt.title('Scatter Plot of Humidity vs Temperature')\n",
							"plt.grid(True, linestyle='--', alpha=0.5)\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 115
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### "
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/batch_processing')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "openweather",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "980803bd-fa7a-469f-ab73-a0d81dd4dffd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/34f89b90-47ae-412e-8758-934b3c3ef8f7/resourceGroups/junhuicho-DS598/providers/Microsoft.Synapse/workspaces/junhuichosynapse/bigDataPools/openweather",
						"name": "openweather",
						"type": "Spark",
						"endpoint": "https://junhuichosynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/openweather",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							"# Define Key Vault Name\n",
							"key_vault_name = \"openweather-junhui\"\n",
							"secret_name = \"storage-key\"  \n",
							"storage_account_name = \"junhuisa\"\n",
							"storage_account_key = mssparkutils.credentials.getSecret(key_vault_name, secret_name)\n",
							"\n",
							"spark.conf.set(\n",
							"    \"fs.azure.account.key.\" + storage_account_name + \".dfs.core.windows.net\",\n",
							"    storage_account_key\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Set up the configuration for accessing the storage account\n",
							"container = \"blob1\"\n",
							"\n",
							"\n",
							"# Define ABFSS path\n",
							"abfss_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
							"\n",
							"# Example: reading a Parquet file from ADLS Gen2\n",
							"air_pollution_df = spark.read.parquet(f\"{abfss_path}silver/historical_airpollution/airpollution_cleaned.parquet\")\n",
							"weather_df = spark.read.parquet(f\"{abfss_path}silver/historical_weather/weather_cleaned.parquet\")\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, round\n",
							"import pyspark.sql.functions as F\n",
							"from pyspark.sql.window import Window\n",
							"from pyspark.sql.types import IntegerType"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"def calculate_aqi(pollutant, concentration):\n",
							"    breakpoints = {\n",
							"        'O3_8hr': [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200), (106, 200, 201, 300)],\n",
							"        'O3_1hr': [(125, 164, 101, 150), (165, 204, 151, 200), (205, 404, 201, 300), (405, 504, 301, 400), (505, 604, 401, 500)],\n",
							"        'PM2.5_24hr': [(0, 12, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 350.4, 301, 400), (350.5, 500.4, 401, 500)],\n",
							"        'PM10_24hr': [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200), (355, 424, 201, 300), (425, 504, 301, 400), (505, 604, 401, 500)],\n",
							"        'CO_8hr': [(0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200), (15.5, 30.4, 201, 300), (30.5, 40.4, 301, 400), (40.5, 50.4, 401, 500)],\n",
							"        'SO2_1hr': [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],\n",
							"        'SO2_24hr': [(305, 604, 201, 300), (605, 804, 301, 400), (805, 1004, 401, 500)],\n",
							"        'NO2_1hr': [(0, 53, 0, 50), (54, 100, 51, 100), (101, 360, 101, 150), (361, 649, 151, 200), (650, 1249, 201, 300), (1250, 2049, 301, 400), (2050, 3049, 401, 500)],\n",
							"    }\n",
							"\n",
							"    if pollutant not in breakpoints:\n",
							"        raise ValueError(f\"Unsupported pollutant: {pollutant}\")\n",
							"\n",
							"    for (Clow, Chigh, Ilow, Ihigh) in breakpoints[pollutant]:\n",
							"        if Clow <= concentration <= Chigh:\n",
							"            return round(((Ihigh - Ilow) / (Chigh - Clow)) * (concentration - Clow) + Ilow)\n",
							"\n",
							"    return None  # If the concentration is out of the given ranges"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"def calculate_rolling_average(df, column, window):\n",
							"    windowSpec = Window.partitionBy(\"location\").orderBy(\"date_time\").rowsBetween(-window+1, 0)\n",
							"    return F.avg(column).over(windowSpec)\n",
							"\n",
							"def calculate_aqi_row(row):\n",
							"    pollutant_map = {\n",
							"        'o3_8hr': 'O3_8hr',\n",
							"        'o3_1hr': 'O3_1hr',\n",
							"        'pm2_5_24hr': 'PM2.5_24hr',\n",
							"        'pm10_24hr': 'PM10_24hr',\n",
							"        'co_8hr': 'CO_8hr',\n",
							"        'so2_1hr': 'SO2_1hr',\n",
							"        'so2_24hr': 'SO2_24hr',\n",
							"        'no2_1hr': 'NO2_1hr'\n",
							"    }\n",
							"    aqi_values = []\n",
							"\n",
							"    for col_name, pollutant in pollutant_map.items():\n",
							"        concentration = row[col_name]\n",
							"        if concentration is not None:\n",
							"            aqi = calculate_aqi(pollutant, concentration)\n",
							"            if aqi is not None:\n",
							"                aqi_values.append(aqi)\n",
							"\n",
							"    if aqi_values:\n",
							"        return max(aqi_values)\n",
							"    return None\n",
							"\n",
							"def calculate_us_aqi(df):\n",
							"    df = df.withColumn(\"o3_8hr\", calculate_rolling_average(df, \"o3\", 8))\n",
							"    df = df.withColumn(\"o3_1hr\", df[\"o3\"])\n",
							"    df = df.withColumn(\"pm2_5_24hr\", calculate_rolling_average(df, \"pm2_5\", 24))\n",
							"    df = df.withColumn(\"pm10_24hr\", calculate_rolling_average(df, \"pm10\", 24))\n",
							"    df = df.withColumn(\"co_8hr\", calculate_rolling_average(df, \"co\", 8))\n",
							"    df = df.withColumn(\"so2_1hr\", df[\"so2\"])\n",
							"    df = df.withColumn(\"so2_24hr\", calculate_rolling_average(df, \"so2\", 24))\n",
							"    df = df.withColumn(\"no2_1hr\", df[\"no2\"])\n",
							"\n",
							"    calculate_aqi_udf = F.udf(lambda row: calculate_aqi_row(row), IntegerType())\n",
							"    \n",
							"    df = df.withColumn(\"us_aqi\", calculate_aqi_udf(F.struct(\n",
							"        col(\"o3_8hr\"),\n",
							"        col(\"o3_1hr\"),\n",
							"        col(\"pm2_5_24hr\"),\n",
							"        col(\"pm10_24hr\"),\n",
							"        col(\"co_8hr\"),\n",
							"        col(\"so2_1hr\"),\n",
							"        col(\"so2_24hr\"),\n",
							"        col(\"no2_1hr\")\n",
							"    )))\n",
							"\n",
							"    return df\n",
							"\n",
							"air_pollution_df = calculate_us_aqi(air_pollution_df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Display selected columns to verify recalculations\n",
							"air_pollution_df.select(\n",
							"    \"date_time\", \n",
							"    \"o3_8hr\", \n",
							"    \"o3_1hr\", \n",
							"    \"pm2_5_24hr\", \n",
							"    \"pm10_24hr\", \n",
							"    \"co_8hr\", \n",
							"    \"so2_1hr\", \n",
							"    \"so2_24hr\", \n",
							"    \"no2_1hr\", \n",
							"    \"us_aqi\",\n",
							"    \"aqi\"\n",
							").show(10)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Add temp_F separately\n",
							"\n",
							"weather_df = weather_df.withColumn(\"temp_F\", round((col(\"temp_K\") - 273.15) * 9 / 5 + 32, 2))\n",
							"\n",
							"weather_df.select(\n",
							"    \"temp_K\",\n",
							"    \"temp_C\",\n",
							"    \"temp_F\"\n",
							").show(10)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Aggregation by Date: Compute daily averages for temperature, humidity, wind speed, etc.\n",
							"weather_agg_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
							"    F.round(F.avg(\"temp_F\")).alias(\"avg_temp_F\"),\n",
							"    F.round(F.avg(\"humidity\"),2).alias(\"avg_humidity\"),\n",
							"    F.round(F.avg(\"wind_speed\"),2).alias(\"avg_wind_speed\"),\n",
							"    F.round(F.max(\"temp_max_F\")).alias(\"max_temp_F\"),\n",
							"    F.round(F.min(\"temp_min_F\")).alias(\"min_temp_F\"),\n",
							"    F.count(\"id\").alias(\"weather_records\")\n",
							")\n",
							"\n",
							"# Weather Condition Counts: Count the occurrences of different weather conditions for each day\n",
							"weather_condition_counts_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\"), \"weather_main_value\").count()\n",
							"\n",
							"# Temperature Extremes: Identify the maximum and minimum temperatures for each day\n",
							"temp_extremes_df = weather_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
							"    F.max(\"temp_max_F\").alias(\"max_temp_F\"),\n",
							"    F.min(\"temp_min_F\").alias(\"min_temp_F\")\n",
							")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Show Daily Average for temp, humidity, wind_speed, temp_max, and temp_min\n",
							"weather_agg_df.orderBy(\"date\").show(truncate=False)\n",
							"\n",
							"# Show Weather Condition Counts\n",
							"weather_condition_counts_df.orderBy(\"date\").show(truncate=False)\n",
							"\n",
							"# Show Temperature Extremes\n",
							"temp_extremes_df.orderBy(\"date\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Daily Average AQI: Compute the daily average Air Quality Index (AQI)\n",
							"aqi_agg_df = air_pollution_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
							"    F.round(F.avg(\"us_aqi\")).alias(\"avg_us_aqi\")\n",
							")\n",
							"\n",
							"# Pollutant Aggregation: Calculate daily averages for each pollutant\n",
							"pollutant_agg_df = air_pollution_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
							"    F.round(F.avg(\"co\"),2).alias(\"avg_co\"),\n",
							"    F.round(F.avg(\"no2\"),2).alias(\"avg_no2\"),\n",
							"    F.round(F.avg(\"o3\"),2).alias(\"avg_o3\"),\n",
							"    F.round(F.avg(\"so2\"),2).alias(\"avg_so2\"),\n",
							"    F.round(F.avg(\"pm2_5\"),2).alias(\"avg_pm2_5\"),\n",
							"    F.round(F.avg(\"pm10\"),2).alias(\"avg_pm10\")\n",
							")\n",
							"\n",
							"# Identify High Pollution Events: Mark days with high pollution levels based on a specified threshold\n",
							"high_pollution_events_df = air_pollution_df.withColumn(\"high_pollution\", F.when(col(\"us_aqi\") > 100, 1).otherwise(0)).groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
							"    F.sum(\"high_pollution\").alias(\"high_pollution_events\")\n",
							")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Show Daily Average AQI\n",
							"aqi_agg_df.orderBy(\"date\").show(truncate=False)\n",
							"\n",
							"# Show Pollutant Daily Averages\n",
							"pollutant_agg_df.orderBy(\"date\").show(truncate=False)\n",
							"\n",
							"# Show High Pollution Events per Day\n",
							"high_pollution_events_df.orderBy(\"date\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"def save_and_rename(df, directory_path, filename):\n",
							"    df.coalesce(1).write.mode(\"overwrite\").parquet(directory_path)\n",
							"\n",
							"    files = mssparkutils.fs.ls(directory_path)\n",
							"    for file in files:\n",
							"        if file.name.startswith(\"part-\") and file.name.endswith(\".parquet\"):\n",
							"            mssparkutils.fs.mv(file.path, directory_path + '/' + filename)\n",
							"            break\n",
							"    \n",
							"    return\n",
							"\n",
							"save_and_rename(weather_agg_df, f\"{abfss_path}/gold/agg_weather\", \"agg_weather.parquet\")\n",
							"save_and_rename(weather_condition_counts_df, f\"{abfss_path}/gold/agg_weather_conditions\", \"agg_weather_conditions.parquet\")\n",
							"save_and_rename(temp_extremes_df, f\"{abfss_path}/gold/agg_temp_extremes\", \"agg_temp_extremes.parquet\")\n",
							"save_and_rename(aqi_agg_df, f\"{abfss_path}/gold/agg_aqi\", \"agg_aqi.parquet\")\n",
							"save_and_rename(pollutant_agg_df, f\"{abfss_path}/gold/agg_pollutants\", \"agg_pollutants.parquet\")\n",
							"save_and_rename(high_pollution_events_df, f\"{abfss_path}/gold/agg_high_pollution_events\", \"agg_high_pollution_events.parquet\")\n",
							"save_and_rename(weather_df, f\"{abfss_path}/gold/processed_weather\", \"processed_weather.parquet\")\n",
							"save_and_rename(air_pollution_df, f\"{abfss_path}/gold/processed_air_pollution\", \"processed_air_pollution.parquet\")\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/openweather')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds598sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}